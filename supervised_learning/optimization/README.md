# Optimization Master

## Project Overview
This project explores various optimization techniques used in machine learning, including hyperparameter tuning, feature scaling, different gradient descent methods, moving averages, learning rate schedules, and batch normalization. The implementation is done using Python 3.5, NumPy 1.15, and TensorFlow 1.12, adhering to the project requirements.

## Learning Objectives
By completing this project, you will:
- Understand and implement various optimization techniques.
- Learn how to normalize input data for better training efficiency.
- Implement different forms of gradient descent, including stochastic and mini-batch gradient descent.
- Explore advanced optimizers like Momentum, RMSProp, and Adam.
- Implement moving averages and learning rate decay.
- Understand and apply batch normalization to neural networks.

## Technologies Used
- **Python 3.5**
- **NumPy 1.15**
- **TensorFlow 1.12** (without Keras)

## Coding Standards
- All code follows **pycodestyle (v2.4)** formatting.
- Each module, class, and function includes appropriate **docstrings**.
- Only `numpy` and `tensorflow` are used; **no external libraries like Keras**.

## Blog Post
I wrote a blog post explaining the mechanics, pros, and cons of the following optimization techniques:

- Feature Scaling
- Batch normalization
- Mini-batch gradient descent
- Gradient descent with momentum
- RMSProp optimization
- Adam optimization
- Learning rate decay
- and more

Kindly peruse the blog post on **"Deep Learning 101: Optimisation in Machine Learning"** [here](https://medium.com/@euniceadewusic/deep-learning-101-optimisation-in-machine-learning-fcee8dea7d67)
